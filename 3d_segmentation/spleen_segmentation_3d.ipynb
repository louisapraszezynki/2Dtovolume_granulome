{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghko6KnqvlY5"
      },
      "source": [
        "Copyright (c) MONAI Consortium  \n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
        "you may not use this file except in compliance with the License.  \n",
        "You may obtain a copy of the License at  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
        "Unless required by applicable law or agreed to in writing, software  \n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
        "See the License for the specific language governing permissions and  \n",
        "limitations under the License.\n",
        "\n",
        "# Spleen 3D segmentation with MONAI\n",
        "\n",
        "This tutorial shows how to integrate MONAI into an existing PyTorch medical DL program.\n",
        "\n",
        "And easily use below features:\n",
        "1. Transforms for dictionary format data.\n",
        "1. Load Nifti image with metadata.\n",
        "1. Add channel dim to the data if no channel dimension.\n",
        "1. Scale medical image intensity with expected range.\n",
        "1. Crop out a batch of balanced images based on positive / negative label ratio.\n",
        "1. Cache IO and transforms to accelerate training and validation.\n",
        "1. 3D UNet model, Dice loss function, Mean Dice metric for 3D segmentation task.\n",
        "1. Sliding window inference method.\n",
        "1. Deterministic training for reproducibility.\n",
        "\n",
        "The Spleen dataset can be downloaded from http://medicaldecathlon.com/.\n",
        "\n",
        "![spleen](https://github.com/Project-MONAI/tutorials/blob/main/figures/spleen0.png?raw=1)\n",
        "\n",
        "Target: Spleen  \n",
        "Modality: CT  \n",
        "Size: 61 3D volumes (41 Training + 20 Testing)  \n",
        "Source: Memorial Sloan Kettering Cancer Center  \n",
        "Challenge: Large ranging foreground size\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/main/3d_segmentation/spleen_segmentation_3d.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn5xVaR-vlY7"
      },
      "source": [
        "## Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LZHT9L_IvlY7",
        "outputId": "3791b029-bf75-4036-b1d7-ac74ba8cadb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "ModuleNotFoundError: No module named 'monai'\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.5/266.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m113.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!python -c \"import monai\" || pip install -q \"monai-weekly[gdown, nibabel, tqdm, ignite]\"\n",
        "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q niicat"
      ],
      "metadata": {
        "id": "W2dpqJSsxm_0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import niicat"
      ],
      "metadata": {
        "id": "D9EYQ63mzVNB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSEi_E9-vlY7"
      },
      "source": [
        "## Setup imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": [],
        "id": "wa4vH5fnvlY8",
        "outputId": "1d66665c-7cb9-43d4-a804-da7997e29361",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MONAI version: 1.5.dev2520\n",
            "Numpy version: 2.0.2\n",
            "Pytorch version: 2.6.0+cu124\n",
            "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
            "MONAI rev id: cc43efa2f524c91686c29a51b04e31e47206dec8\n",
            "MONAI __file__: /usr/local/lib/python3.11/dist-packages/monai/__init__.py\n",
            "\n",
            "Optional dependencies:\n",
            "Pytorch Ignite version: 0.4.11\n",
            "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "Nibabel version: 5.3.2\n",
            "scikit-image version: 0.25.2\n",
            "scipy version: 1.15.3\n",
            "Pillow version: 11.2.1\n",
            "Tensorboard version: 2.18.0\n",
            "gdown version: 5.2.0\n",
            "TorchVision version: 0.21.0+cu124\n",
            "tqdm version: 4.67.1\n",
            "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "psutil version: 5.9.5\n",
            "pandas version: 2.2.2\n",
            "einops version: 0.8.1\n",
            "transformers version: 4.51.3\n",
            "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "\n",
            "For details about installing the optional dependencies, please visit:\n",
            "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from monai.utils import first, set_determinism\n",
        "from monai.transforms import (\n",
        "    AsDiscrete,\n",
        "    AsDiscreted,\n",
        "    EnsureChannelFirstd,\n",
        "    Compose,\n",
        "    CropForegroundd,\n",
        "    LoadImaged,\n",
        "    Orientationd,\n",
        "    RandCropByPosNegLabeld,\n",
        "    SaveImaged,\n",
        "    ScaleIntensityRanged,\n",
        "    Spacingd,\n",
        "    Invertd,\n",
        ")\n",
        "from monai.handlers.utils import from_engine\n",
        "from monai.networks.nets import UNet\n",
        "from monai.networks.layers import Norm\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.losses import DiceLoss\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch\n",
        "from monai.config import print_config\n",
        "from monai.apps import download_and_extract\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import tempfile\n",
        "import shutil\n",
        "import os\n",
        "import glob\n",
        "\n",
        "print_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDap6BYwvlY8"
      },
      "source": [
        "## Setup data directory\n",
        "\n",
        "You can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable.  \n",
        "This allows you to save results and reuse downloads.  \n",
        "If not specified a temporary directory will be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "tags": [],
        "id": "NGpE5rQBvlY9",
        "outputId": "60536903-9278-4424-b624-baa4ecbc3ee8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/tmprk750aea\n"
          ]
        }
      ],
      "source": [
        "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
        "if directory is not None:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
        "print(root_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ob2hoUivlY9"
      },
      "source": [
        "## Download dataset\n",
        "\n",
        "Downloads and extracts the dataset.  \n",
        "The dataset comes from http://medicaldecathlon.com/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "tags": [],
        "id": "pOF0VSFTvlY9",
        "outputId": "4f76910e-70a3-41ed-a352-f38fe9ffca7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Task09_Spleen.tar: 1.50GB [01:31, 17.6MB/s]                            "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-22 08:47:36,043 - INFO - Downloaded: /tmp/tmprk750aea/Task09_Spleen.tar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-22 08:47:38,918 - INFO - Verified 'Task09_Spleen.tar', md5: 410d4a301da4e5b2f6f86ec3ddba524e.\n",
            "2025-05-22 08:47:38,919 - INFO - Writing into directory: /tmp/tmprk750aea.\n"
          ]
        }
      ],
      "source": [
        "resource = \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task09_Spleen.tar\"\n",
        "md5 = \"410d4a301da4e5b2f6f86ec3ddba524e\"\n",
        "\n",
        "compressed_file = os.path.join(root_dir, \"Task09_Spleen.tar\")\n",
        "data_dir = os.path.join(root_dir, \"Task09_Spleen\")\n",
        "if not os.path.exists(data_dir):\n",
        "    download_and_extract(resource, compressed_file, root_dir, md5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YApK1XVEvlY9"
      },
      "source": [
        "## Set MSD Spleen dataset path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1O_vHKUAvlY9",
        "outputId": "71b444fb-0ef2-4e04-fb49-caa4e7296d82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/tmprk750aea/Task09_Spleen/imagesTr/spleen_12.nii.gz\n"
          ]
        }
      ],
      "source": [
        "train_images = sorted(glob.glob(os.path.join(data_dir, \"imagesTr\", \"*.nii.gz\")))\n",
        "train_labels = sorted(glob.glob(os.path.join(data_dir, \"labelsTr\", \"*.nii.gz\")))\n",
        "data_dicts = [{\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(train_images, train_labels)]\n",
        "train_files, val_files = data_dicts[:-9], data_dicts[-9:]\n",
        "\n",
        "print(train_images[1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nibabel as nib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Charger l'image\n",
        "img_path = './tmp/tmpwj1f40lm/Task09_Spleen/imagesTr/spleen_10.nii.gz'\n",
        "img = nib.load(img_path)\n",
        "img_data = img.get_fdata()\n",
        "\n",
        "# Afficher une slice axiale (au milieu du volume)\n",
        "slice_index = img_data.shape[2] // 2\n",
        "plt.imshow(img_data[:, :, slice_index], cmap='gray')\n",
        "plt.title(f\"Slice {slice_index}\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XDPKQ5_iyp4c",
        "outputId": "e14412b6-76c3-4938-d2bf-4bd18604fe4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "No such file or no access: 'tmp/tmpwj1f40lm/Task09_Spleen/imagesTr/spleen_10.nii.gz'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nibabel/loadsave.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mstat_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tmp/tmpwj1f40lm/Task09_Spleen/imagesTr/spleen_10.nii.gz'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c2a8fb98968d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Charger l'image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./tmp/tmpwj1f40lm/Task09_Spleen/imagesTr/spleen_10.nii.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mimg_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nibabel/loadsave.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mstat_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No such file or no access: '{filename}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstat_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_size\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mImageFileError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Empty file: '{filename}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or no access: 'tmp/tmpwj1f40lm/Task09_Spleen/imagesTr/spleen_10.nii.gz'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51oVYqwovlY9"
      },
      "source": [
        "## Set deterministic training for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9c-Qx0kvlY9"
      },
      "outputs": [],
      "source": [
        "set_determinism(seed=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YoAMr1NvlY-"
      },
      "source": [
        "## Setup transforms for training and validation\n",
        "\n",
        "Here we use several transforms to augment the dataset:\n",
        "1. `LoadImaged` loads the spleen CT images and labels from NIfTI format files.\n",
        "1. `EnsureChannelFirstd` ensures the original data to construct \"channel first\" shape.\n",
        "1. `Orientationd` unifies the data orientation based on the affine matrix.\n",
        "1. `Spacingd` adjusts the spacing by `pixdim=(1.5, 1.5, 2.)` based on the affine matrix.\n",
        "1. `ScaleIntensityRanged` extracts intensity range [-57, 164] and scales to [0, 1].\n",
        "1. `CropForegroundd` removes all zero borders to focus on the valid body area of the images and labels.\n",
        "1. `RandCropByPosNegLabeld` randomly crop patch samples from big image based on pos / neg ratio.  \n",
        "The image centers of negative samples must be in valid body area.\n",
        "1. `RandAffined` efficiently performs `rotate`, `scale`, `shear`, `translate`, etc. together based on PyTorch affine transform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae4pqFcKvlY-"
      },
      "outputs": [],
      "source": [
        "train_transforms = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        ScaleIntensityRanged(\n",
        "            keys=[\"image\"],\n",
        "            a_min=-57,\n",
        "            a_max=164,\n",
        "            b_min=0.0,\n",
        "            b_max=1.0,\n",
        "            clip=True,\n",
        "        ),\n",
        "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\", allow_smaller=True),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        Spacingd(keys=[\"image\", \"label\"], pixdim=(1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
        "        RandCropByPosNegLabeld(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            label_key=\"label\",\n",
        "            spatial_size=(96, 96, 96),\n",
        "            pos=1,\n",
        "            neg=1,\n",
        "            num_samples=4,\n",
        "            image_key=\"image\",\n",
        "            image_threshold=0,\n",
        "        ),\n",
        "        # user can also add other random transforms\n",
        "        # RandAffined(\n",
        "        #     keys=['image', 'label'],\n",
        "        #     mode=('bilinear', 'nearest'),\n",
        "        #     prob=1.0, spatial_size=(96, 96, 96),\n",
        "        #     rotate_range=(0, 0, np.pi/15),\n",
        "        #     scale_range=(0.1, 0.1, 0.1)),\n",
        "    ]\n",
        ")\n",
        "val_transforms = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        ScaleIntensityRanged(\n",
        "            keys=[\"image\"],\n",
        "            a_min=-57,\n",
        "            a_max=164,\n",
        "            b_min=0.0,\n",
        "            b_max=1.0,\n",
        "            clip=True,\n",
        "        ),\n",
        "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\", allow_smaller=True),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        Spacingd(keys=[\"image\", \"label\"], pixdim=(1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_qMFNM4vlY-"
      },
      "source": [
        "## Check transforms in DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "AKgwLXTpvlY-"
      },
      "outputs": [],
      "source": [
        "check_ds = Dataset(data=val_files, transform=val_transforms)\n",
        "check_loader = DataLoader(check_ds, batch_size=1)\n",
        "check_data = first(check_loader)\n",
        "image, label = (check_data[\"image\"][0][0], check_data[\"label\"][0][0])\n",
        "print(f\"image shape: {image.shape}, label shape: {label.shape}\")\n",
        "# plot the slice [:, :, 80]\n",
        "plt.figure(\"check\", (12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"image\")\n",
        "plt.imshow(image[:, :, 80], cmap=\"gray\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"label\")\n",
        "plt.imshow(label[:, :, 80])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIaTNFKEvlY-"
      },
      "source": [
        "## Define CacheDataset and DataLoader for training and validation\n",
        "\n",
        "Here we use CacheDataset to accelerate training and validation process, it's 10x faster than the regular Dataset.  \n",
        "To achieve best performance, set `cache_rate=1.0` to cache all the data, if memory is not enough, set lower value.  \n",
        "Users can also set `cache_num` instead of `cache_rate`, will use the minimum value of the 2 settings.  \n",
        "And set `num_workers` to enable multi-threads during caching.  \n",
        "If want to to try the regular Dataset, just change to use the commented code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "uvDKph76vlY-"
      },
      "outputs": [],
      "source": [
        "train_ds = CacheDataset(data=train_files, transform=train_transforms, cache_rate=1.0, num_workers=4)\n",
        "# train_ds = Dataset(data=train_files, transform=train_transforms)\n",
        "\n",
        "# use batch_size=2 to load images and use RandCropByPosNegLabeld\n",
        "# to generate 2 x 4 images for network training\n",
        "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=4)\n",
        "\n",
        "val_ds = CacheDataset(data=val_files, transform=val_transforms, cache_rate=1.0, num_workers=4)\n",
        "# val_ds = Dataset(data=val_files, transform=val_transforms)\n",
        "val_loader = DataLoader(val_ds, batch_size=1, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlUv6M4ivlY-"
      },
      "source": [
        "## Create Model, Loss, Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TtMWSrqvlY-"
      },
      "outputs": [],
      "source": [
        "# standard PyTorch program style: create UNet, DiceLoss and Adam optimizer\n",
        "device = torch.device(\"cuda:0\")\n",
        "model = UNet(\n",
        "    spatial_dims=3,\n",
        "    in_channels=1,\n",
        "    out_channels=2,\n",
        "    channels=(16, 32, 64, 128, 256),\n",
        "    strides=(2, 2, 2, 2),\n",
        "    num_res_units=2,\n",
        "    norm=Norm.BATCH,\n",
        ").to(device)\n",
        "loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
        "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geiYWzfrvlY-"
      },
      "source": [
        "## Execute a typical PyTorch training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "HMz7XaesvlY_"
      },
      "outputs": [],
      "source": [
        "max_epochs = 600\n",
        "val_interval = 2\n",
        "best_metric = -1\n",
        "best_metric_epoch = -1\n",
        "epoch_loss_values = []\n",
        "metric_values = []\n",
        "post_pred = Compose([AsDiscrete(argmax=True, to_onehot=2)])\n",
        "post_label = Compose([AsDiscrete(to_onehot=2)])\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    print(\"-\" * 10)\n",
        "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    step = 0\n",
        "    for batch_data in train_loader:\n",
        "        step += 1\n",
        "        inputs, labels = (\n",
        "            batch_data[\"image\"].to(device),\n",
        "            batch_data[\"label\"].to(device),\n",
        "        )\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        print(f\"{step}/{len(train_ds) // train_loader.batch_size}, \" f\"train_loss: {loss.item():.4f}\")\n",
        "    epoch_loss /= step\n",
        "    epoch_loss_values.append(epoch_loss)\n",
        "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    if (epoch + 1) % val_interval == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for val_data in val_loader:\n",
        "                val_inputs, val_labels = (\n",
        "                    val_data[\"image\"].to(device),\n",
        "                    val_data[\"label\"].to(device),\n",
        "                )\n",
        "                roi_size = (160, 160, 160)\n",
        "                sw_batch_size = 4\n",
        "                val_outputs = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model)\n",
        "                val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n",
        "                val_labels = [post_label(i) for i in decollate_batch(val_labels)]\n",
        "                # compute metric for current iteration\n",
        "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
        "\n",
        "            # aggregate the final mean dice result\n",
        "            metric = dice_metric.aggregate().item()\n",
        "            # reset the status for next validation round\n",
        "            dice_metric.reset()\n",
        "\n",
        "            metric_values.append(metric)\n",
        "            if metric > best_metric:\n",
        "                best_metric = metric\n",
        "                best_metric_epoch = epoch + 1\n",
        "                torch.save(model.state_dict(), os.path.join(root_dir, \"best_metric_model.pth\"))\n",
        "                print(\"saved new best metric model\")\n",
        "            print(\n",
        "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
        "                f\"\\nbest mean dice: {best_metric:.4f} \"\n",
        "                f\"at epoch: {best_metric_epoch}\"\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "DWO23oAHvlY_",
        "outputId": "bd5697d8-58da-4af3-9f69-33f2ec12a05a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train completed, best_metric: 0.9510 at epoch: 598\n"
          ]
        }
      ],
      "source": [
        "print(f\"train completed, best_metric: {best_metric:.4f} \" f\"at epoch: {best_metric_epoch}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnaihhPMvlY_"
      },
      "source": [
        "## Plot the loss and metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtzLePD5vlY_"
      },
      "outputs": [],
      "source": [
        "plt.figure(\"train\", (12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Epoch Average Loss\")\n",
        "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
        "y = epoch_loss_values\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(x, y)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Val Mean Dice\")\n",
        "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
        "y = metric_values\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(x, y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-XcwvxPvlY_"
      },
      "source": [
        "## Check best model output with the input image and label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHTbCh2TvlY_"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\"), weights_only=True))\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i, val_data in enumerate(val_loader):\n",
        "        roi_size = (160, 160, 160)\n",
        "        sw_batch_size = 4\n",
        "        val_outputs = sliding_window_inference(val_data[\"image\"].to(device), roi_size, sw_batch_size, model)\n",
        "        # plot the slice [:, :, 80]\n",
        "        plt.figure(\"check\", (18, 6))\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.title(f\"image {i}\")\n",
        "        plt.imshow(val_data[\"image\"][0, 0, :, :, 80], cmap=\"gray\")\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.title(f\"label {i}\")\n",
        "        plt.imshow(val_data[\"label\"][0, 0, :, :, 80])\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.title(f\"output {i}\")\n",
        "        plt.imshow(torch.argmax(val_outputs, dim=1).detach().cpu()[0, :, :, 80])\n",
        "        plt.show()\n",
        "        if i == 2:\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_OYyTDQvlY_"
      },
      "source": [
        "## Evaluation on original image spacings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cn6PUxJ4vlY_"
      },
      "outputs": [],
      "source": [
        "val_org_transforms = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
        "        Spacingd(keys=[\"image\"], pixdim=(1.5, 1.5, 2.0), mode=\"bilinear\"),\n",
        "        ScaleIntensityRanged(\n",
        "            keys=[\"image\"],\n",
        "            a_min=-57,\n",
        "            a_max=164,\n",
        "            b_min=0.0,\n",
        "            b_max=1.0,\n",
        "            clip=True,\n",
        "        ),\n",
        "        CropForegroundd(keys=[\"image\"], source_key=\"image\", allow_smaller=True),\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_org_ds = Dataset(data=val_files, transform=val_org_transforms)\n",
        "val_org_loader = DataLoader(val_org_ds, batch_size=1, num_workers=4)\n",
        "\n",
        "post_transforms = Compose(\n",
        "    [\n",
        "        Invertd(\n",
        "            keys=\"pred\",\n",
        "            transform=val_org_transforms,\n",
        "            orig_keys=\"image\",\n",
        "            meta_keys=\"pred_meta_dict\",\n",
        "            orig_meta_keys=\"image_meta_dict\",\n",
        "            meta_key_postfix=\"meta_dict\",\n",
        "            nearest_interp=False,\n",
        "            to_tensor=True,\n",
        "            device=\"cpu\",\n",
        "        ),\n",
        "        AsDiscreted(keys=\"pred\", argmax=True, to_onehot=2),\n",
        "        AsDiscreted(keys=\"label\", to_onehot=2),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cj4GuE2uvlY_"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\"), weights_only=True))\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for val_data in val_org_loader:\n",
        "        val_inputs = val_data[\"image\"].to(device)\n",
        "        roi_size = (160, 160, 160)\n",
        "        sw_batch_size = 4\n",
        "        val_data[\"pred\"] = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model)\n",
        "        val_data = [post_transforms(i) for i in decollate_batch(val_data)]\n",
        "        val_outputs, val_labels = from_engine([\"pred\", \"label\"])(val_data)\n",
        "        # compute metric for current iteration\n",
        "        dice_metric(y_pred=val_outputs, y=val_labels)\n",
        "\n",
        "    # aggregate the final mean dice result\n",
        "    metric_org = dice_metric.aggregate().item()\n",
        "    # reset the status for next validation round\n",
        "    dice_metric.reset()\n",
        "\n",
        "print(\"Metric on original image spacing: \", metric_org)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIGYJaZJvlY_"
      },
      "source": [
        "## Inference on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1Kzm8WOvlZA"
      },
      "outputs": [],
      "source": [
        "test_images = sorted(glob.glob(os.path.join(data_dir, \"imagesTs\", \"*.nii.gz\")))\n",
        "\n",
        "test_data = [{\"image\": image} for image in test_images]\n",
        "\n",
        "\n",
        "test_org_transforms = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=\"image\"),\n",
        "        EnsureChannelFirstd(keys=\"image\"),\n",
        "        Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
        "        Spacingd(keys=[\"image\"], pixdim=(1.5, 1.5, 2.0), mode=\"bilinear\"),\n",
        "        ScaleIntensityRanged(\n",
        "            keys=[\"image\"],\n",
        "            a_min=-57,\n",
        "            a_max=164,\n",
        "            b_min=0.0,\n",
        "            b_max=1.0,\n",
        "            clip=True,\n",
        "        ),\n",
        "        CropForegroundd(keys=[\"image\"], source_key=\"image\", allow_smaller=True),\n",
        "    ]\n",
        ")\n",
        "\n",
        "test_org_ds = Dataset(data=test_data, transform=test_org_transforms)\n",
        "\n",
        "test_org_loader = DataLoader(test_org_ds, batch_size=1, num_workers=4)\n",
        "\n",
        "post_transforms = Compose(\n",
        "    [\n",
        "        Invertd(\n",
        "            keys=\"pred\",\n",
        "            transform=test_org_transforms,\n",
        "            orig_keys=\"image\",\n",
        "            meta_keys=\"pred_meta_dict\",\n",
        "            orig_meta_keys=\"image_meta_dict\",\n",
        "            meta_key_postfix=\"meta_dict\",\n",
        "            nearest_interp=False,\n",
        "            to_tensor=True,\n",
        "        ),\n",
        "        AsDiscreted(keys=\"pred\", argmax=True, to_onehot=2),\n",
        "        SaveImaged(keys=\"pred\", meta_keys=\"pred_meta_dict\", output_dir=\"./out\", output_postfix=\"seg\", resample=False),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJLPxpINvlZA"
      },
      "outputs": [],
      "source": [
        "# uncomment the following lines to visualize the predicted results\n",
        "from monai.transforms import LoadImage\n",
        "loader = LoadImage()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Djtiw5R0vlZA"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\"), weights_only=True))\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for test_data in test_org_loader:\n",
        "        test_inputs = test_data[\"image\"].to(device)\n",
        "        roi_size = (160, 160, 160)\n",
        "        sw_batch_size = 4\n",
        "        test_data[\"pred\"] = sliding_window_inference(test_inputs, roi_size, sw_batch_size, model)\n",
        "\n",
        "        test_data = [post_transforms(i) for i in decollate_batch(test_data)]\n",
        "\n",
        "         # uncomment the following lines to visualize the predicted results\n",
        "         test_output = from_engine([\"pred\"])(test_data)\n",
        "\n",
        "         original_image = loader(test_output[0].meta[\"filename_or_obj\"])\n",
        "\n",
        "         plt.figure(\"check\", (18, 6))\n",
        "         plt.subplot(1, 2, 1)\n",
        "         plt.imshow(original_image[:, :, 20], cmap=\"gray\")\n",
        "         plt.subplot(1, 2, 2)\n",
        "         plt.imshow(test_output[0].detach().cpu()[1, :, :, 20])\n",
        "         plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLKxSLywvlZA"
      },
      "source": [
        "## Cleanup data directory\n",
        "\n",
        "Remove directory if a temporary was used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fV3XAOW_vlZA"
      },
      "outputs": [],
      "source": [
        "if directory is None:\n",
        "    shutil.rmtree(root_dir)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}